{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Model Training: Key Parameters and Configurations  : SSD_Lite_320(MobileNET_V1 / MobileNET_V2 Backbone )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training Hyperparameters  \n",
    "\n",
    "| **Parameter**       | **Description**                        | **Typical Values**         |\n",
    "|---------------------|------------------------------------|---------------------------|\n",
    "| **Batch Size**      | Number of samples per batch        | 8, 16, 32                 |\n",
    "| **Learning Rate**   | Initial learning rate             | 0.001, 0.0001, 0.005      |\n",
    "| **Optimizer**       | Optimization algorithm            | SGD, Adam, AdamW, RMSprop |\n",
    "| **Momentum**        | Momentum for SGD optimizer        | 0.9, 0.95                 |\n",
    "| **Weight Decay**    | L2 regularization term            | 0.0005, 0.001             |\n",
    "| **Learning Rate Scheduler** | Schedule for adjusting learning rate | CosineAnnealingLR, ReduceLROnPlateau, MultiStepLR |\n",
    "| **Epochs**         | Number of training iterations     | 50, 100, 200              |\n",
    "\n",
    "## 2. Loss Function Parameters  \n",
    "\n",
    "The model employs a **multi-task loss function** consisting of classification and localization losses. Key parameter to tune:\n",
    "\n",
    "- **`alpha`**: Weighting factor between classification and localization losses (default: `1.0`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.io.image import decode_image\n",
    "from torchvision.models.detection import (\n",
    "    ssdlite320_mobilenet_v3_large,\n",
    "    SSDLite320_MobileNet_V3_Large_Weights,\n",
    ")\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\PyTorch_GPU\\torch_gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SSDLite320_MobileNet_V3_Large_Weights.COCO_V1`. You can also use `weights=SSDLite320_MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[  6.0037,   3.6826, 315.0123, 316.9638],\n",
      "        [  5.1216,   3.1682, 315.6669, 317.4081],\n",
      "        [211.2740, 165.9436, 218.6624, 180.2623],\n",
      "        ...,\n",
      "        [108.9881,  29.2311, 125.4374,  44.8632],\n",
      "        [ 50.6379,  71.6034,  61.0590,  81.9207],\n",
      "        [  6.3974,   0.0000, 314.6940, 320.0000]], grad_fn=<StackBackward0>), 'scores': tensor([0.0640, 0.0457, 0.0395, 0.0381, 0.0372, 0.0370, 0.0364, 0.0361, 0.0359,\n",
      "        0.0358, 0.0358, 0.0356, 0.0356, 0.0355, 0.0353, 0.0351, 0.0351, 0.0351,\n",
      "        0.0349, 0.0349, 0.0348, 0.0347, 0.0347, 0.0346, 0.0344, 0.0344, 0.0343,\n",
      "        0.0343, 0.0341, 0.0341, 0.0338, 0.0336, 0.0336, 0.0335, 0.0335, 0.0335,\n",
      "        0.0334, 0.0334, 0.0333, 0.0333, 0.0332, 0.0332, 0.0331, 0.0331, 0.0331,\n",
      "        0.0330, 0.0329, 0.0328, 0.0328, 0.0328, 0.0327, 0.0327, 0.0326, 0.0326,\n",
      "        0.0325, 0.0324, 0.0322, 0.0322, 0.0321, 0.0321, 0.0320, 0.0320, 0.0320,\n",
      "        0.0320, 0.0318, 0.0317, 0.0317, 0.0317, 0.0317, 0.0317, 0.0317, 0.0317,\n",
      "        0.0316, 0.0316, 0.0316, 0.0316, 0.0316, 0.0316, 0.0316, 0.0314, 0.0313,\n",
      "        0.0313, 0.0311, 0.0310, 0.0310, 0.0310, 0.0310, 0.0310, 0.0309, 0.0309,\n",
      "        0.0309, 0.0308, 0.0308, 0.0308, 0.0307, 0.0307, 0.0307, 0.0307, 0.0306,\n",
      "        0.0306, 0.0304, 0.0303, 0.0303, 0.0303, 0.0303, 0.0302, 0.0301, 0.0301,\n",
      "        0.0301, 0.0301, 0.0301, 0.0301, 0.0300, 0.0300, 0.0300, 0.0298, 0.0298,\n",
      "        0.0298, 0.0298, 0.0298, 0.0297, 0.0297, 0.0297, 0.0296, 0.0296, 0.0295,\n",
      "        0.0294, 0.0294, 0.0294, 0.0294, 0.0294, 0.0293, 0.0293, 0.0293, 0.0293,\n",
      "        0.0293, 0.0293, 0.0292, 0.0292, 0.0292, 0.0292, 0.0292, 0.0291, 0.0291,\n",
      "        0.0291, 0.0290, 0.0290, 0.0290, 0.0289, 0.0289, 0.0289, 0.0289, 0.0289,\n",
      "        0.0289, 0.0288, 0.0288, 0.0288, 0.0288, 0.0287, 0.0287, 0.0287, 0.0287,\n",
      "        0.0287, 0.0287, 0.0286, 0.0285, 0.0285, 0.0285, 0.0285, 0.0284, 0.0284,\n",
      "        0.0284, 0.0284, 0.0283, 0.0283, 0.0283, 0.0283, 0.0283, 0.0282, 0.0282,\n",
      "        0.0282, 0.0282, 0.0281, 0.0280, 0.0280, 0.0279, 0.0279, 0.0279, 0.0279,\n",
      "        0.0279, 0.0278, 0.0278, 0.0278, 0.0277, 0.0277, 0.0276, 0.0276, 0.0276,\n",
      "        0.0276, 0.0276, 0.0275, 0.0275, 0.0275, 0.0275, 0.0274, 0.0274, 0.0274,\n",
      "        0.0274, 0.0273, 0.0273, 0.0273, 0.0273, 0.0273, 0.0272, 0.0272, 0.0272,\n",
      "        0.0272, 0.0272, 0.0271, 0.0271, 0.0271, 0.0271, 0.0270, 0.0270, 0.0270,\n",
      "        0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270,\n",
      "        0.0269, 0.0269, 0.0269, 0.0268, 0.0268, 0.0268, 0.0268, 0.0268, 0.0268,\n",
      "        0.0268, 0.0267, 0.0267, 0.0267, 0.0267, 0.0267, 0.0267, 0.0267, 0.0266,\n",
      "        0.0266, 0.0265, 0.0265, 0.0265, 0.0265, 0.0265, 0.0264, 0.0264, 0.0264,\n",
      "        0.0264, 0.0264, 0.0264, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0262,\n",
      "        0.0262, 0.0262, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261, 0.0261,\n",
      "        0.0261, 0.0261, 0.0261, 0.0260, 0.0260, 0.0260, 0.0260, 0.0260, 0.0260,\n",
      "        0.0259, 0.0259, 0.0259, 0.0259, 0.0259, 0.0259, 0.0259, 0.0259, 0.0259,\n",
      "        0.0258, 0.0258, 0.0205], grad_fn=<IndexBackward0>), 'labels': tensor([ 1, 63, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 67, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 65])}, {'boxes': tensor([[ 10.2386,   6.0543, 388.5111, 495.7127],\n",
      "        [  5.4219,   9.9351, 393.7404, 491.9634],\n",
      "        [ 10.3972,   1.8760, 388.3211, 498.7565],\n",
      "        ...,\n",
      "        [ 38.6240,  68.5484,  54.4846, 105.0911],\n",
      "        [ 22.5116, 128.8003, 189.6328, 500.0000],\n",
      "        [184.4683, 152.8459, 200.1873, 185.5442]], grad_fn=<StackBackward0>), 'scores': tensor([0.0457, 0.0456, 0.0287, 0.0257, 0.0246, 0.0246, 0.0245, 0.0244, 0.0243,\n",
      "        0.0240, 0.0240, 0.0235, 0.0235, 0.0234, 0.0233, 0.0231, 0.0230, 0.0227,\n",
      "        0.0226, 0.0225, 0.0224, 0.0223, 0.0223, 0.0221, 0.0221, 0.0220, 0.0220,\n",
      "        0.0219, 0.0218, 0.0218, 0.0217, 0.0215, 0.0215, 0.0214, 0.0214, 0.0214,\n",
      "        0.0213, 0.0213, 0.0210, 0.0210, 0.0209, 0.0208, 0.0207, 0.0206, 0.0206,\n",
      "        0.0204, 0.0204, 0.0203, 0.0202, 0.0201, 0.0200, 0.0200, 0.0198, 0.0198,\n",
      "        0.0198, 0.0198, 0.0198, 0.0197, 0.0196, 0.0195, 0.0195, 0.0194, 0.0194,\n",
      "        0.0194, 0.0193, 0.0193, 0.0192, 0.0191, 0.0191, 0.0190, 0.0190, 0.0190,\n",
      "        0.0190, 0.0190, 0.0190, 0.0190, 0.0190, 0.0189, 0.0188, 0.0188, 0.0188,\n",
      "        0.0188, 0.0187, 0.0186, 0.0186, 0.0184, 0.0184, 0.0183, 0.0183, 0.0182,\n",
      "        0.0181, 0.0181, 0.0181, 0.0181, 0.0180, 0.0179, 0.0179, 0.0179, 0.0179,\n",
      "        0.0179, 0.0178, 0.0178, 0.0178, 0.0178, 0.0177, 0.0177, 0.0176, 0.0176,\n",
      "        0.0176, 0.0176, 0.0175, 0.0175, 0.0175, 0.0175, 0.0175, 0.0174, 0.0174,\n",
      "        0.0174, 0.0174, 0.0173, 0.0173, 0.0173, 0.0173, 0.0173, 0.0173, 0.0171,\n",
      "        0.0171, 0.0170, 0.0170, 0.0170, 0.0170, 0.0170, 0.0169, 0.0169, 0.0169,\n",
      "        0.0169, 0.0168, 0.0168, 0.0168, 0.0168, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0166, 0.0166, 0.0166, 0.0166,\n",
      "        0.0166, 0.0166, 0.0165, 0.0165, 0.0165, 0.0165, 0.0164, 0.0164, 0.0163,\n",
      "        0.0163, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163,\n",
      "        0.0163, 0.0162, 0.0162, 0.0162, 0.0162, 0.0161, 0.0161, 0.0161, 0.0161,\n",
      "        0.0161, 0.0160, 0.0160, 0.0160, 0.0160, 0.0160, 0.0160, 0.0160, 0.0160,\n",
      "        0.0160, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0159, 0.0158, 0.0158,\n",
      "        0.0158, 0.0157, 0.0157, 0.0157, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156,\n",
      "        0.0155, 0.0155, 0.0155, 0.0155, 0.0155, 0.0154, 0.0154, 0.0154, 0.0153,\n",
      "        0.0153, 0.0153, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
      "        0.0151, 0.0151, 0.0151, 0.0151, 0.0151, 0.0151, 0.0151, 0.0150, 0.0150,\n",
      "        0.0150, 0.0150, 0.0150, 0.0150, 0.0150, 0.0150, 0.0150, 0.0149, 0.0149,\n",
      "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0148,\n",
      "        0.0148, 0.0148, 0.0148, 0.0148, 0.0148, 0.0148, 0.0148, 0.0148, 0.0147,\n",
      "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0146,\n",
      "        0.0146, 0.0146, 0.0146, 0.0146, 0.0145, 0.0145, 0.0145, 0.0145, 0.0144,\n",
      "        0.0144, 0.0144, 0.0144, 0.0144, 0.0144, 0.0144, 0.0144, 0.0144, 0.0144,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143], grad_fn=<IndexBackward0>), 'labels': tensor([63,  1, 67, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 65, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16,  1, 16, 16, 16, 16, 16, 16, 16, 16, 28, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,  1, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 38, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 38, 16, 16, 16, 38, 16, 16,\n",
      "        16, 16, 16, 16, 16, 38, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 38, 16, 16, 16, 16, 16, 16, 38, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 38,  1, 16])}]\n"
     ]
    }
   ],
   "source": [
    "model = ssdlite320_mobilenet_v3_large(\n",
    "    weights=ssdlite320_mobilenet_v3_large, progress=True, num_classes=91\n",
    ")\n",
    "model.eval()\n",
    "x = [torch.rand(3, 320, 320), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<property at 0x1ca5c036d90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_for_ssdlite_320 = SSDLite320_MobileNet_V3_Large_Weights\n",
    "auto_transforms_ssdlite_3320 = weights_for_ssdlite_320.transforms\n",
    "auto_transforms_ssdlite_3320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "SSD                                                          [300, 4]                  --\n",
       "├─GeneralizedRCNNTransform: 1-1                              [1, 3, 320, 320]          --\n",
       "├─SSDLiteFeatureExtractorMobileNet: 1-2                      [1, 128, 1, 1]            --\n",
       "│    └─Sequential: 2-1                                       --                        --\n",
       "│    │    └─Sequential: 3-1                                  [1, 672, 20, 20]          869,096\n",
       "│    │    └─Sequential: 3-2                                  [1, 480, 10, 10]          751,416\n",
       "│    └─ModuleList: 2-2                                       --                        --\n",
       "│    │    └─Sequential: 3-3                                  [1, 512, 5, 5]            258,304\n",
       "│    │    └─Sequential: 3-4                                  [1, 256, 3, 3]            100,480\n",
       "│    │    └─Sequential: 3-5                                  [1, 256, 2, 2]            67,712\n",
       "│    │    └─Sequential: 3-6                                  [1, 128, 1, 1]            25,664\n",
       "├─SSDLiteHead: 1-3                                           [1, 3234, 91]             --\n",
       "│    └─SSDLiteRegressionHead: 2-3                            [1, 3234, 4]              --\n",
       "│    │    └─ModuleList: 3-7                                  --                        80,784\n",
       "│    └─SSDLiteClassificationHead: 2-4                        [1, 3234, 91]             --\n",
       "│    │    └─ModuleList: 3-8                                  --                        1,286,604\n",
       "├─DefaultBoxGenerator: 1-4                                   [3234, 4]                 --\n",
       "==============================================================================================================\n",
       "Total params: 3,440,060\n",
       "Trainable params: 3,440,060\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 583.52\n",
       "==============================================================================================================\n",
       "Input size (MB): 1.23\n",
       "Forward/backward pass size (MB): 153.46\n",
       "Params size (MB): 13.76\n",
       "Estimated Total Size (MB): 168.45\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(1, 3, 320, 320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================================================================================================\n",
       "Layer (type (var_name))                                                Input Shape          Output Shape         Param #              Trainable\n",
       "======================================================================================================================================================\n",
       "SSD (SSD)                                                              [32, 3, 224, 224]    [300, 4]             --                   True\n",
       "├─GeneralizedRCNNTransform (transform)                                 [32, 3, 224, 224]    [32, 3, 320, 320]    --                   --\n",
       "├─SSDLiteFeatureExtractorMobileNet (backbone)                          [32, 3, 320, 320]    [32, 128, 1, 1]      --                   True\n",
       "│    └─Sequential (features)                                           --                   --                   --                   True\n",
       "│    │    └─Sequential (0)                                             [32, 3, 320, 320]    [32, 672, 20, 20]    869,096              True\n",
       "│    │    └─Sequential (1)                                             [32, 672, 20, 20]    [32, 480, 10, 10]    751,416              True\n",
       "│    └─ModuleList (extra)                                              --                   --                   --                   True\n",
       "│    │    └─Sequential (0)                                             [32, 480, 10, 10]    [32, 512, 5, 5]      258,304              True\n",
       "│    │    └─Sequential (1)                                             [32, 512, 5, 5]      [32, 256, 3, 3]      100,480              True\n",
       "│    │    └─Sequential (2)                                             [32, 256, 3, 3]      [32, 256, 2, 2]      67,712               True\n",
       "│    │    └─Sequential (3)                                             [32, 256, 2, 2]      [32, 128, 1, 1]      25,664               True\n",
       "├─SSDLiteHead (head)                                                   [32, 672, 20, 20]    [32, 3234, 91]       --                   True\n",
       "│    └─SSDLiteRegressionHead (regression_head)                         [32, 672, 20, 20]    [32, 3234, 4]        --                   True\n",
       "│    │    └─ModuleList (module_list)                                   --                   --                   80,784               True\n",
       "│    └─SSDLiteClassificationHead (classification_head)                 [32, 672, 20, 20]    [32, 3234, 91]       --                   True\n",
       "│    │    └─ModuleList (module_list)                                   --                   --                   1,286,604            True\n",
       "├─DefaultBoxGenerator (anchor_generator)                               [32, 3, 320, 320]    [3234, 4]            --                   --\n",
       "======================================================================================================================================================\n",
       "Total params: 3,440,060\n",
       "Trainable params: 3,440,060\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 18.67\n",
       "======================================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 4910.83\n",
       "Params size (MB): 13.76\n",
       "Estimated Total Size (MB): 4943.85\n",
       "======================================================================================================================================================"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(\n",
    "    model=model,\n",
    "    input_size=(32, 3, 224, 224),  # make sure this is \"input_size\", not \"input_shape\"\n",
    "    # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real_time inference on Laptop webcam using SSD_lite model on CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F\n",
    "import time\n",
    "from coco_classes import COCO_CLASSES\n",
    "\n",
    "# Load the pretrained model with GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(pretrained=True).to(\n",
    "    device\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "# Set desired frame size (model expects 320x320)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1080)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Warmup GPU\n",
    "x = torch.randn(1, 3, 320, 320).to(device)\n",
    "_ = model(x)\n",
    "\n",
    "while True:\n",
    "    # Read frame from webcam\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess frame\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = F.to_tensor(img).to(device)\n",
    "    img = F.resize(img, (320, 320))  # Resize to model input size\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        predictions = model([img])[0]\n",
    "        inference_time = time.time() - start_time\n",
    "\n",
    "    # Filter predictions with confidence > 0.5\n",
    "    mask = predictions[\"scores\"] > 0.5\n",
    "    boxes = predictions[\"boxes\"][mask].cpu().numpy()\n",
    "    labels = predictions[\"labels\"][mask].cpu().numpy()\n",
    "    scores = predictions[\"scores\"][mask].cpu().numpy()\n",
    "\n",
    "    # Draw predictions\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "        # Scale boxes back to original frame size\n",
    "        scale_x = frame.shape[1] / 320\n",
    "        scale_y = frame.shape[0] / 320\n",
    "        x1 = int(x1 * scale_x)\n",
    "        y1 = int(y1 * scale_y)\n",
    "        x2 = int(x2 * scale_x)\n",
    "        y2 = int(y2 * scale_y)\n",
    "\n",
    "        # Draw rectangle and label\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        label_text = f\"{COCO_CLASSES[label]}: {score:.2f}\"\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            label_text,\n",
    "            (x1, y1 - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (0, 255, 0),\n",
    "            2,\n",
    "        )\n",
    "\n",
    "    # Display FPS\n",
    "    fps_text = f\"FPS: {1 / inference_time:.2f}\"\n",
    "    cv2.putText(frame, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv2.imshow(\"Real-Time Detection\", frame)\n",
    "\n",
    "    # Exit on 'q' press\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Achieving 20FPS with NVIDIA-CUDA GPU Support , very promising results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
